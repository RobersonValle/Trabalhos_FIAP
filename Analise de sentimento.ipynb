{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import unidecode\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_columns  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "ds_encoding = \"ISO-8859-1\"\n",
    "df_tweet_original = pd.read_csv(r'D:\\Estudos\\FIAP\\Fase 5\\Tweets.csv', sep=',', encoding=ds_encoding, names=ds_columns)\n",
    "df_tweet_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = df_tweet_original.sample(2000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetorizar = CountVectorizer(lowercase=False, max_features=2000)\n",
    "bag_of_words = vetorizar.fit_transform(df_tweet.text)\n",
    "# print(bag_of_words)\n",
    "matriz_esparsa = pd.DataFrame.sparse.from_spmatrix(bag_of_words, columns=vetorizar.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matriz_esparsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificacao_texto(df, coluna_texto, coluna_classificacao):\n",
    "    vetorizar = CountVectorizer(lowercase=False, max_features=2000)\n",
    "    bag_of_words = vetorizar.fit_transform(df[coluna_texto])\n",
    "    # matriz_esparsa = pd.DataFrame.sparse.from_spmatrix(bag_of_words, columns=vetorizar.get_feature_names())\n",
    "    \n",
    "    treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words, \n",
    "                                                                    df[coluna_classificacao], \n",
    "                                                                    random_state=42)\n",
    "    regressao_logistica = LogisticRegression()\n",
    "    regressao_logistica.fit(treino,classe_treino)\n",
    "    return regressao_logistica.score(teste, classe_teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificacao_texto(df_tweet,'text','sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_df = list()\n",
    "for frase in df_tweet.text:\n",
    "    frase_teste = []\n",
    "    for palavra in frase.split():\n",
    "        if '@' in palavra or '.com' in palavra:\n",
    "            continue\n",
    "        else:\n",
    "            frase_teste.append(palavra)\n",
    "    lista_df.append(' '.join(frase_teste))\n",
    "\n",
    "df_tweet['text_v2'] = lista_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = Translator()\n",
    "frase_traduzida = list()\n",
    "for frase in tqdm(df_tweet.text_v2):\n",
    "    try:\n",
    "        frase_traduzida.append(trans.translate(frase, dest='pt').text)\n",
    "    except:\n",
    "        frase_traduzida.append('')\n",
    "df_tweet['text_pt'] = frase_traduzida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.to_csv('arquivo_traduzido.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificacao_texto(df_tweet,'text_pt','sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuvem_palavras(texto, coluna_texto):\n",
    "    todas_palavras = ' ' .join([texto for texto in texto[coluna_texto]])\n",
    "    nuvem_palavras = WordCloud(width=400, height=250, max_font_size=50, collocations=False).generate(todas_palavras)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(nuvem_palavras,interpolation='bilinear')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuvem_palavras(df_tweet, 'text_pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todas_palavras = ' ' .join([texto for texto in df_tweet.text_pt])\n",
    "token_espaco = tokenize.WhitespaceTokenizer()\n",
    "token_frase = token_espaco.tokenize(todas_palavras)\n",
    "frequencia = nltk.FreqDist(token_frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencia = pd.DataFrame({\n",
    "                    'Palavra':list(frequencia.keys()),\n",
    "                    'Frequencia':list(frequencia.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencia.nlargest(columns='Frequencia',n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto(texto, coluna_texto, quantidade):\n",
    "  todas_palavras = ' '.join([resenha for resenha in texto[coluna_texto]])\n",
    "  token_espaco = nltk.tokenize.WhitespaceTokenizer()\n",
    "  token_frase = token_espaco.tokenize(todas_palavras)\n",
    "  frequencias = nltk.FreqDist(token_frase)\n",
    "  df_frequencias = pd.DataFrame({'Palavras': list(frequencias.keys()),\n",
    "                               'Frequencia': list(frequencias.values())})\n",
    "  df_frequencias = df_frequencias.nlargest(n=quantidade, columns='Frequencia')\n",
    "\n",
    "  total = df_frequencias['Frequencia'].sum()\n",
    "  df_frequencias['Porcentagem'] = df_frequencias['Frequencia'].cumsum() / total * 100\n",
    "\n",
    "  plt.figure(figsize=(12,8))\n",
    "  ax = sns.barplot(data=df_frequencias, x='Palavras', y='Frequencia', color='gray')\n",
    "  ax2 = ax.twinx()\n",
    "  sns.lineplot(data=df_frequencias, x='Palavras', y='Porcentagem', color='red', sort=False, ax=ax2)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto(df_tweet,'text_pt',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_irrelevantes = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "tweet_processado = list()\n",
    "for tweet in df_tweet.text_pt:\n",
    "    novo_tweet = list()\n",
    "    palavras_texto = token_espaco.tokenize(tweet)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in palavras_irrelevantes:\n",
    "            novo_tweet.append(palavra)\n",
    "    tweet_processado.append(' '.join(novo_tweet))\n",
    "\n",
    "df_tweet['text_pt_v2'] = tweet_processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificacao_texto(df_tweet, 'text_pt_v2', 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto(df_tweet,'text_pt_v2',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pontuacao = tokenize.WordPunctTokenizer()\n",
    "pontuacao = list()\n",
    "for ponto in punctuation:\n",
    "    pontuacao.append(ponto)\n",
    "\n",
    "pontuacao_stopwords = pontuacao + palavras_irrelevantes\n",
    "\n",
    "tweet_processado = list()\n",
    "for tweet in df_tweet['text_pt_v2']:\n",
    "    novo_tweet = list()\n",
    "    palavras_texto = token_pontuacao.tokenize(tweet)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in pontuacao_stopwords:\n",
    "            novo_tweet.append(palavra)\n",
    "    tweet_processado.append(' '.join(novo_tweet))\n",
    "\n",
    "df_tweet['text_pt_v3'] = tweet_processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificacao_texto(df_tweet, 'text_pt_v3', 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_acentos = [unidecode.unidecode(texto) for texto in df_tweet.text_pt_v3]\n",
    "stopwords = [unidecode.unidecode(texto) for texto in pontuacao_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['text_pt_v4'] = sem_acentos\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "tweet_processado = list()\n",
    "for tweet in df_tweet.text_pt_v4:\n",
    "    novo_tweet = list()\n",
    "    tweet = tweet.lower()\n",
    "    palavras_texto = token_pontuacao.tokenize(tweet)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in pontuacao_stopwords:\n",
    "            novo_tweet.append(stemmer.stem(palavra))\n",
    "    tweet_processado.append(' '.join(novo_tweet))\n",
    "\n",
    "df_tweet['text_pt_v4'] = tweet_processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificacao_texto(df_tweet, 'text_pt_v4', 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto(df_tweet, 'text_pt_v4',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codigo para teste, ainda preciso entender melhor como implementar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "frases_teste = ['Assisti um filme ótimo','Assisti um filme péssivo']\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False, max_features=50)\n",
    "\n",
    "caracteristicas = tfidf.fit_transform(frases_teste)\n",
    "pd.DataFrame(\n",
    "    caracteristicas.todense(),\n",
    "    columns=tfidf.get_feature_names()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "frases = 'Assisti um ótimo filme.'\n",
    "frase_separada = token_espaco.tokenize(frases)\n",
    "pares = ngrams(frase_separada,2)\n",
    "list(pares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False, ngram_range=(1,2))\n",
    "vetor_tfidf = tfidf.fit_transform(df_tweet['text_pt'])\n",
    "treino, teste, classe_treino,classe_teste = train_test_split(vetor_tfidf,\n",
    "                                                            df_tweet['sentiment'],\n",
    "                                                            random_state=42)\n",
    "regressao_logistica = LogisticRegression()\n",
    "regressao_logistica.fit(treino,classe_treino)\n",
    "regressao_logistica.score(teste, classe_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False, ngram_range=(1,2))\n",
    "vetor_tfidf = tfidf.fit_transform(df_tweet['text_pt_v4'])\n",
    "treino, teste, classe_treino,classe_teste = train_test_split(vetor_tfidf,\n",
    "                                                            df_tweet['sentiment'],\n",
    "                                                            random_state=42)\n",
    "regressao_logistica = LogisticRegression()\n",
    "regressao_logistica.fit(treino,classe_treino)\n",
    "regressao_logistica.score(teste, classe_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90215180e3510b7cc610bbcd7049faaefdb75cf213a625a3b7913872d73ad565"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
