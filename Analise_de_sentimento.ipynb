{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import unidecode\n",
    "from string import punctuation\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_columns  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "ds_encoding = \"ISO-8859-1\"\n",
    "df_tweet = pd.read_csv(r'D:\\Estudos\\FIAP\\Fase 5\\Trabalhos_FIAP\\arquivo_traduzido.csv', sep=',')\n",
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetorizar = CountVectorizer(lowercase=False, max_features=50)\n",
    "bag_of_words = vetorizar.fit_transform(df_tweet.text_pt)\n",
    "# print(bag_of_words)\n",
    "# matriz_esparsa = pd.DataFrame.sparse.from_spmatrix(bag_of_words, columns=vetorizar.get_feature_names())\n",
    "# print(matriz_esparsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificacao_texto(df, coluna_texto, coluna_classificacao):\n",
    "    vetorizar = CountVectorizer(lowercase=False, max_features=2000)\n",
    "    bag_of_words = vetorizar.fit_transform(df[coluna_texto])\n",
    "    # matriz_esparsa = pd.DataFrame.sparse.from_spmatrix(bag_of_words, columns=vetorizar.get_feature_names())\n",
    "    \n",
    "    treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words, \n",
    "                                                                    df[coluna_classificacao], \n",
    "                                                                    random_state=42)\n",
    "    regressao_logistica = LogisticRegression()\n",
    "    regressao_logistica.fit(treino,classe_treino)\n",
    "    return regressao_logistica.score(teste, classe_teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "freq_tweets = vectorizer.fit_transform(df_tweet.text_pt)\n",
    "\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(freq_tweets, df_tweet.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testes = [\"Esse governo está no início, vamos ver o que vai dar\",\n",
    "          \"Estou muito feliz com o governo de São Paulo esse ano\",\n",
    "          \"O estado de Minas Gerais decretou calamidade financeira!!!\",\n",
    "          \"A segurança desse país está deixando a desejar\",\n",
    "          \"O governador de Minas é do PT\",\n",
    "          \"O prefeito de São Paulo está fazendo um ótimo trabalho\"]\n",
    "\n",
    "freq_testes = vectorizer.transform(testes)\n",
    "modelo.predict(freq_testes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = cross_val_predict(modelo, freq_tweets, df_tweet.sentiment, cv = 10)\n",
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(df_tweet.sentiment, resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto(texto, coluna_texto, quantidade):\n",
    "  todas_palavras = ' '.join([resenha for resenha in texto[coluna_texto]])\n",
    "  token_espaco = nltk.tokenize.WhitespaceTokenizer()\n",
    "  token_frase = token_espaco.tokenize(todas_palavras)\n",
    "  frequencias = nltk.FreqDist(token_frase)\n",
    "  df_frequencias = pd.DataFrame({'Palavras': list(frequencias.keys()),\n",
    "                               'Frequencia': list(frequencias.values())})\n",
    "  df_frequencias = df_frequencias.nlargest(n=quantidade, columns='Frequencia')\n",
    "\n",
    "  total = df_frequencias['Frequencia'].sum()\n",
    "  df_frequencias['Porcentagem'] = df_frequencias['Frequencia'].cumsum() / total * 100\n",
    "\n",
    "  plt.figure(figsize=(12,8))\n",
    "  ax = sns.barplot(data=df_frequencias, x='Palavras', y='Frequencia', color='gray')\n",
    "  ax2 = ax.twinx()\n",
    "  sns.lineplot(data=df_frequencias, x='Palavras', y='Porcentagem', color='red', sort=False, ax=ax2)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto(df_tweet,'text_pt',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuvem_palavras(texto, coluna_texto):\n",
    "    todas_palavras = ' ' .join([texto for texto in texto[coluna_texto]])\n",
    "    nuvem_palavras = WordCloud(width=400, height=250, max_font_size=50, collocations=False).generate(todas_palavras)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(nuvem_palavras,interpolation='bilinear')\n",
    "    plt.show()\n",
    "nuvem_palavras(df_tweet, 'text_pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todas_palavras = ' ' .join([texto for texto in df_tweet.text_pt])\n",
    "token_espaco = tokenize.WhitespaceTokenizer()\n",
    "token_frase = token_espaco.tokenize(todas_palavras)\n",
    "frequencia = nltk.FreqDist(token_frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencia = pd.DataFrame({\n",
    "                    'Palavra':list(frequencia.keys()),\n",
    "                    'Frequencia':list(frequencia.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencia.nlargest(columns='Frequencia',n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_irrelevantes = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "tweet_processado = list()\n",
    "for tweet in df_tweet.text_pt:\n",
    "    novo_tweet = list()\n",
    "    palavras_texto = token_espaco.tokenize(tweet)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in palavras_irrelevantes:\n",
    "            novo_tweet.append(palavra)\n",
    "    tweet_processado.append(' '.join(novo_tweet))\n",
    "\n",
    "df_tweet['text_pt_v2'] = tweet_processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pontuacao = tokenize.WordPunctTokenizer()\n",
    "pontuacao = list()\n",
    "for ponto in punctuation:\n",
    "    pontuacao.append(ponto)\n",
    "\n",
    "pontuacao_stopwords = pontuacao + palavras_irrelevantes\n",
    "\n",
    "tweet_processado = list()\n",
    "for tweet in df_tweet['text_pt_v2']:\n",
    "    novo_tweet = list()\n",
    "    palavras_texto = token_pontuacao.tokenize(tweet)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in pontuacao_stopwords:\n",
    "            novo_tweet.append(palavra)\n",
    "    tweet_processado.append(' '.join(novo_tweet))\n",
    "\n",
    "df_tweet['text_pt_v3'] = tweet_processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_acentos = [unidecode.unidecode(texto) for texto in df_tweet.text_pt_v3]\n",
    "stopwords = [unidecode.unidecode(texto) for texto in pontuacao_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['text_pt_v4'] = sem_acentos\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "tweet_processado = list()\n",
    "for tweet in df_tweet.text_pt_v4:\n",
    "    novo_tweet = list()\n",
    "    tweet = tweet.lower()\n",
    "    palavras_texto = token_pontuacao.tokenize(tweet)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in pontuacao_stopwords:\n",
    "            novo_tweet.append(stemmer.stem(palavra))\n",
    "    tweet_processado.append(' '.join(novo_tweet))\n",
    "\n",
    "df_tweet['text_pt_v4'] = tweet_processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto(df_tweet,'text_pt_v4',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuvem_palavras(df_tweet, 'text_pt_v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_irrelevantes_v2 = ['agor','aind', '..', '...', 'dia','tod','faz','tao','noit', \n",
    "                            'quer','pod', 'nov','esper','!!','cas','enta','hoj','poss',\n",
    "                            'ach','apen','vai','vou','2','volt','est','ter','twitt','acab',\n",
    "                            'temp','seman','assist','voc','ir','algum','!!!','dev','sab','falt','cois','trabalh']\n",
    "\n",
    "tweet_processado = []\n",
    "for tweet in df_tweet.text_pt_v4:\n",
    "    novo_tweet = []\n",
    "    palavras_texto = token_espaco.tokenize(tweet)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in palavras_irrelevantes_v2:\n",
    "            novo_tweet.append(palavra)\n",
    "    tweet_processado.append(' '.join(novo_tweet))\n",
    "\n",
    "df_tweet['text_pt_v5'] = tweet_processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto(df_tweet,'text_pt_v5',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Acuracia sem tratamento: {classificacao_texto(df_tweet, 'text_pt', 'sentiment')}\")\n",
    "print(f\"Acuracia após o 1º tratamento: {classificacao_texto(df_tweet, 'text_pt_v2', 'sentiment')}\")\n",
    "print(f\"Acuracia após o 2º tratamento: {classificacao_texto(df_tweet, 'text_pt_v3', 'sentiment')}\")\n",
    "print(f\"Acuracia após o 3º tratamento: {classificacao_texto(df_tweet, 'text_pt_v4', 'sentiment')}\")\n",
    "print(f\"Acuracia após o 4º tratamento: {classificacao_texto(df_tweet, 'text_pt_v5', 'sentiment')}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90215180e3510b7cc610bbcd7049faaefdb75cf213a625a3b7913872d73ad565"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
